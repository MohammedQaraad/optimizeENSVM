# -*- coding: utf-8 -*-
"""ELSVMElasticNetCV Final .ipynb

Automatically generated by Colaboratory.
@author: Mohammed Qaraad 

Original file is located at
    https://colab.research.google.com/drive/1chCqhh5aen_qbi8Q1bZiBIbm22_RSCsI
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
import csv
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
import random
from google.colab import drive
drive.mount('/content/drive')
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
from sklearn.linear_model import ElasticNet , ElasticNetCV
from collections import Counter
import tarfile
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
from sklearn.metrics import roc_curve, auc
from numpy import interp
from sklearn.model_selection import RandomizedSearchCV
from sklearn.utils.fixes import loguniform

from  warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)
random_s = np.random.RandomState(10)

# Hyper-Parameter for optimize SVM C and gamma 
cMin=0.01
cMax=1000
gMin=0.0001
gMax=0.1
#lower and upper Velocity
alpha=0.1
vcMax = alpha *(cMax - cMin)
vcMin= - vcMax
vgMax= alpha * (gMax - gMin)
vgMin= - vgMax
verbose=0


random_s = np.random.RandomState(0)

def initPosition(Np,Nd,cMax, cMin,gMax,gMin):
    random_s = np.random.RandomState(0)

    R=np.zeros((Np,Nd))
    for p in range (0,Np):  #10
      #R = initPosition (R, Np, Nd, cMin, cMax, gMax, gMin)
      R[p][0]= np.random.rand()*(cMax-cMin)+cMin  #  Penalty parameter (C)
      R[p][1]= np.random.rand()*(gMax-gMin)+gMin #  RBF Kernel (Sigma)
    
    return R


def initVelocity(Np,Nd):
    v=np.zeros((Np,Nd))
    for p in range (0,Np):  #10
      #R = initPosition (R, Np, Nd, cMin, cMax, gMax, gMin)
      v[p][0]= 0
      v[p][1]= 0
    
    return v


def perf_measure(y_actual, y_hat):
    TP = 0
    FP = 0
    TN = 0
    FN = 0

    for i in range(len(y_hat)): 
        if y_actual[i]==y_hat[i]==1:
           TP += 1
        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:
           FP += 1
        if y_actual[i]==y_hat[i]==0:
           TN += 1
        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:
           FN += 1

    return(TP, FP, TN, FN)

"""# **ELENSVM with FS**"""



# 'golub.tar.gz','gordon.tar.gz','singh.tar.gz','west.tar.gz'
# 'chin.tar.gz','alon.tar.gz','chowdary.tar.gz'
# ,'singh.tar.gz','west.tar.gz'
# 'gordon.tar.gz','golub.tar.gz'
data_set_name=['singh.tar.gz']
ŸêAUCagg = pd.DataFrame()
plt.figure(figsize=(8,8))
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',
         label='Luck', alpha=.8)    
import time
start = time.time()
for i in data_set_name:
  d= i.split('.')[0]
  print(d)
  file='/content/drive/My Drive/Colab Notebooks/data/'+i
  datasetname=d

  min_max_scaler = MinMaxScaler()
  with tarfile.open(file, "r:*") as tar:
    csv_path = tar.getnames()
    if(d == 'singh' or d == 'west'):
      labels = pd.read_csv(tar.extractfile(csv_path[1]), header=None)
      feature = pd.read_csv(tar.extractfile(csv_path[0]), header=None)

    else:
      labels = pd.read_csv(tar.extractfile(csv_path[0]), header=None)
      feature = pd.read_csv(tar.extractfile(csv_path[1]), header=None)
  headers = list(feature.columns)
  features = np.asarray(feature.values)
  features = min_max_scaler.fit_transform(feature)
  labels = np.transpose(np.asarray(labels.values.ravel() - 1, dtype=int))
  print(features.shape, labels.shape)

  counter = Counter(labels)
  print(counter)
  
  Max_iterations=50  # Maximum Number of Iterations
  correction_factor = 2.0 # Correction factor
  inertia = 1.0 # Ineritia Coeffecient
  swarm_size = 20 # Number of particles
  LB=1e-4*np.ones((1,2))
  UB=1e-3*np.ones((1,2))

  Xrange=UB[0,0]-LB[0,0]
  Yrange=UB[0,1]-LB[0,1]
  Dim=UB.shape[1]

  fold_N=0  # Number of runs
  cv = StratifiedKFold(n_splits=10,shuffle=False)

  #evaluated matrix
  max_fold=10
  Accuracy=np.zeros((max_fold))
  Precision=np.zeros((max_fold))
  Recall=np.zeros((max_fold))
  ResultsTestingAcc = np.zeros((max_fold))
  TP=[]
  FP=[]
  TN=[]
  FN=[]

  Specificity = np.zeros((max_fold))
  Sensitifity = np.zeros((max_fold))
  AUC = np.zeros((max_fold))

  tprs = []
  aucs = []
  mean_fpr = np.linspace(0, 1, 100)
  BestAlpha=np.zeros((max_fold))

  NoRun=1
  
  for r in range(0,NoRun):
    fold_N=0
    i=0
    lines=["-",":","--","-."]
    for train_index, test_index in cv.split(features,labels):
      Swarm=np.zeros((swarm_size,2))
      PreviouBest=float('inf')* np.ones((Max_iterations,3))
      # Initial best value so far
      # GlobalBestSolution=float('inf')
      # BestSolns=float('inf')* np.ones((swarm_size,2))
      gBestPos   = [0] * 1
      gBestValue = float("inf") 
      #store Global Best Position and Solution For Current Run 
    
	    
      # Initial velocity
      # Velocity=np.zeros((swarm_size,2))
      Fval=np.zeros((Max_iterations))
      Xtrain, Xtest =  features[train_index], features[test_index]
      ytrain, ytest = labels[train_index], labels[test_index]

      

      for iter in range(Max_iterations):
        # Calculating fitness value for all particles    
        #print('Iteration Number ', iter )
       
        for i in range(swarm_size):
                      
          #R = initPosition (R, Np, Nd, cMin, cMax, gMax, gMin)
          Swarm[i,0]= np.random.random()*Xrange+LB[0,0]  #  Penalty parameter (C)
          Swarm[i,1]= np.random.random()*Yrange+LB[0,1] #  RBF Kernel (Sigma)
          
          Swarm[i,0] = max(Swarm[i,0],LB[0,0])
          Swarm[i,0] = min(Swarm[i,0],UB[0,0])
          Swarm[i,1] = max(Swarm[i,1],LB[0,1])
          Swarm[i,1] = min(Swarm[i,1],UB[0,1])
      
        #clf = GridSearchCV(estimator=svm.SVC(),cv=1, param_grid=parameter_candidates, n_jobs=-1)
        clf = ElasticNetCV(alphas=list(Swarm[:,0]), tol=0.0001, cv=None, max_iter=1,
                           copy_X=True, verbose=0, n_jobs=-1, positive=False, random_state=random_s, selection='cyclic')
        # clf = RandomizedSearchCV(ElasticNet(), parameter_grid, n_iter=1, scoring='r2', n_jobs=-1, random_state=random_s)
        #clf.fit(x_train, y_train)
        clf.fit(Xtrain, ytrain) 
        #claculate Fitness 
        #print("iteration ", iter , 'Param', clf.best_estimator_.alpha)
        elasticNet=ElasticNet(alpha=clf.alpha_,random_state=random_s)
        elasticNet.fit(Xtrain,ytrain)
        data_el= elasticNet.coef_
        gains = np.asarray(data_el)
        indexes = np.where(gains != 0)[0]
        #print(indexes.shape)
        if(indexes.shape[0]== 0):
          continue

        xtrain = Xtrain[:, indexes]
        xtest = Xtest[:, indexes]
        #xtrain, xvalidation, ytrain, yvalidation = train_test_split(xtrain,y_train,test_size = 0.11)



        svclassifier = SVC(kernel='rbf',verbose=0, random_state=random_s)  
        svclassifier.fit(xtrain, ytrain)
        ypred = svclassifier.predict(xtest)
            
        ValidationError=sum(ypred==ytest)*100/xtest.shape[0]
        Fval[iter]= 100 - ValidationError

            
        if Fval[iter]<PreviouBest[iter,2]:
          PreviouBest[iter,0]=clf.alpha_# Update the position of the first dimension
          PreviouBest[iter,1]=0 # Update the position of the second dimension
          PreviouBest[iter,2]=Fval[iter]          # Update best value

        #calculate global solu and position for current Run 
        if Fval[iter] < gBestValue:
          gBestValue = Fval[iter]
          gBestPos   = clf.alpha_  




      print('fold_N = ', fold_N , ' Best value of Alpha = ', gBestPos,   'with fitness value = ', gBestValue)
      BestAlpha[fold_N]= gBestPos
      #ResultsValidation[r][fold_N]=gBestValue
      #fold_best_postion[fold_N]= gBestPos

      F_train, F_test =  features[train_index], features[test_index]
      z_train, z_test = labels[train_index], labels[test_index]

      #claculate Fitness 
      elasticNet=ElasticNet(alpha=gBestPos,random_state=random_s)
      elasticNet.fit(F_train,z_train)
      data_el= elasticNet.coef_
      gains = np.asarray(data_el)
      indexes = np.where(gains != 0)[0]
      #print(indexes.shape)
      if(indexes.shape[0]== 0):
        continue

      f_train = F_train[:, indexes]
      f_test = F_test[:, indexes]
    
      svclassifier = SVC(kernel='rbf', verbose=0, random_state=random_s)
      svclassifier.fit(f_train, z_train)
      ypred = svclassifier.predict(f_test)
      #y_true = y_test
      TestingError=sum(ypred==z_test)*100/z_test.shape[0]
      print('TestingError = ', TestingError)
      ResultsTestingAcc[fold_N]=100-TestingError;
      #SV[r]= svclassifier.n_support_
      #print('fold_N testing error ', 100-TestingError)
      fpr, tpr, t = roc_curve(z_test, ypred)
      #print('fpr ',fpr) 
      #print('tpr',tpr)
      tprs.append(interp(mean_fpr, fpr, tpr))
      roc_auc = auc(fpr, tpr)
      aucs.append(roc_auc)
      # i= i+1
      conf_matrix = confusion_matrix(z_test, ypred)
      # TP.append(conf_matrix[1][1])
      # TN.append(conf_matrix[0][0])
      # FP.append(conf_matrix[0][1])
      # FN.append(conf_matrix[1][0])
      # conf_sensitivity = (conf_matrix[1][1] / float(conf_matrix[1][1] + conf_matrix[1][0]))
      # TPR=conf_sensitivity
      sensitivity1 = conf_matrix[0,0]/(conf_matrix[0,0]+conf_matrix[0,1])

      Sensitifity[fold_N]=sensitivity1
      # conf_specificity = (conf_matrix[0][0] / float(conf_matrix[0][0] + conf_matrix[0][1]))
      # FPR= (1 - conf_specificity)
      specificity1 = conf_matrix[1,1]/(conf_matrix[1,0]+conf_matrix[1,1])

      Specificity[fold_N]=specificity1
      # auC= (float(1+TPR - FPR)/2)
      AUC[fold_N]=roc_auc
      fold_N = fold_N +1
      

  
      
  
    print (d, "AUC = %.2f SenStd = %.2f " % (AUC.mean(), AUC.std()))
    print (d, "Sen = %.2f SenStd = %.2f " % (Sensitifity.mean(), Sensitifity.std()))
    print (d, "Spe = %.2f SenStd = %.2f " % (Specificity.mean(), Specificity.std()))
    print('AUC' ,AUC)
    print('Sensitifity', Sensitifity)
    print('Specificity' ,Specificity)
    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    std_auc = np.std(aucs)
    if(i% 2 == 0):
      plt.plot(mean_fpr, mean_tpr,
               label=d +' Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc),
               lw=2, alpha=.8, linestyle=lines[i])
    else:
      plt.plot(mean_fpr, mean_tpr,
               label=d +' Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc),
               lw=2, alpha=.8)

    i = i +1

    std_tpr = np.std(tprs, axis=0)
    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2)
print('******************************************')
end = time.time()
temp = end-start
print(temp)
hours = temp//3600
temp = temp - 3600*hours
minutes = temp//60
seconds = temp - 60*minutes
print('%d:%d:%d' %(hours,minutes,seconds))
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("RandomizedSearchCV RS-ENSVM results  ")
plt.legend(loc="lower right")
plt.show()



"""# **RandomizedsearchCV without FS optimize RBF kernel**"""

# ,'choewdary.tar.gz','golub.tar.gz','gordon.tar.gz'+
data_set_name=['chin.tar.gz','alon.tar.gz','chowdary.tar.gz','golub.tar.gz','gordon.tar.gz',
               'singh.tar.gz','west.tar.gz']
plt.figure(figsize=(8,8))
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',
         label='Luck', alpha=.8)    
import time
start = time.time()
for i in data_set_name:
  d= i.split('.')[0]
  print(d)
  file='/content/drive/My Drive/Colab Notebooks/data/'+i
  datasetname=d

  min_max_scaler = MinMaxScaler()
  with tarfile.open(file, "r:*") as tar:
    csv_path = tar.getnames()
    if(d == 'singh' or d == 'west'):
      labels = pd.read_csv(tar.extractfile(csv_path[1]), header=None)
      feature = pd.read_csv(tar.extractfile(csv_path[0]), header=None)

    else:
      labels = pd.read_csv(tar.extractfile(csv_path[0]), header=None)
      feature = pd.read_csv(tar.extractfile(csv_path[1]), header=None)
  headers = list(feature.columns)
  features = np.asarray(feature.values)
  features = min_max_scaler.fit_transform(feature)
  labels = np.transpose(np.asarray(labels.values.ravel() - 1, dtype=int))
  print(features.shape, labels.shape)

  counter = Counter(labels)
  print(counter)
  Max_iterations=50
  LB=1e-4*np.ones((1,2))
  UB=1e-3*np.ones((1,2))

  Xrange=UB[0,0]-LB[0,0]
  Yrange=UB[0,1]-LB[0,1]
  Dim=UB.shape[1]


  NoRun=1
  cv = StratifiedKFold(n_splits=10,shuffle=False)
  fold_N=0
  max_fold=10
  Nd=2  
  Np = 20
  maxIter = 50;
  target_error=0.0 
  
  ConvergenceCurve=np.zeros((maxIter))
  Gbest = np.zeros((maxIter))
  best = np.zeros((maxIter,Nd))
  ResultsValidation = np.zeros((NoRun,max_fold))
  ResultsTestingAcc = np.zeros((NoRun,max_fold))
  fold_best_postion =np.zeros((max_fold,2))
  #evaluated matrix 
  Accuracy=np.zeros((max_fold))
  Precision=np.zeros((max_fold))
  Recall=np.zeros((max_fold))
  TP=[]
  FP=[]
  TN=[]
  FN=[]

  Specificity = np.zeros((max_fold))
  Sensitifity = np.zeros((max_fold))
  AUC = np.zeros((max_fold))
  X= features
  y=labels
  tprs = []
  aucs = []
  mean_fpr = np.linspace(0, 1, 100)

  
  for r in range(0,NoRun):
    fold_N=0
    i=0
    for train_index, test_index in cv.split(features,labels):
      w=1;                # Inertia Weight
      wdamp=0.98;         # Inertia Weight Damping Ratio
      correction_factor1 = 2;
      correction_factor2 = 2;
      kernel='rbf';
      verbose=0;
      swarm_size=20
      Swarm=np.zeros((swarm_size,2))
      PreviouBest=float('inf')* np.ones((Max_iterations,3))
      # Initial best value so far
      # GlobalBestSolution=float('inf')
      # BestSolns=float('inf')* np.ones((swarm_size,2))
      gBestPos   = [0] * 2
      gBestValue = float("inf") 
    
      # Initialize the population/solutions
      R = initPosition(Np,Nd,cMax, cMin,gMax,gMin)
      V = initVelocity(Np,Nd)
      #initialization Fitness function
      Fval=np.zeros((Max_iterations))
      #Data Partition 
      X_train, X_test =  X[train_index], X[test_index]
      y_train, y_test = y[train_index], y[test_index]
      x_train, x_validation, y_train, y_validation = train_test_split(X_train,y_train,test_size = 0.11)

      # #evaluate Fintness Function 
      # for p in range (0,Np):
      #   svclassifier = SVC(kernel='rbf', gamma = R[p][1], C = R[p][0] ,verbose=0, random_state=random_s)
      #   svclassifier.fit(x_train, y_train)
      #   ypred = svclassifier.predict(x_validation)
      #   ValidationError=sum(ypred==y_validation)*100/y_validation.shape[0]
      #   #print('y_validation.shape[0]', y_validation.shape[0], 'sum(ypred==y_validation)', sum(ypred==y_validation), 'ValidationError', ValidationError)
      #   fitness[p]= 100 - ValidationError
      
      #   if fitness[p] < pBestValue[p]:
      #     pBestValue[p] = fitness[p]
      #     pBestPos[p][0]   = R[p][0]
      #     pBestPos[p][1]   = R[p][1]

      #   if fitness[p] < gBestValue:
      #     gBestValue = fitness[p]
      #     gBestPos   = R[p]  

      for iter in range(0, maxIter):
        # Swarm= initPosition(Np,Nd,cMax, cMin,gMax,gMin)
        for i in range(swarm_size):
            
          #R = initPosition (R, Np, Nd, cMin, cMax, gMax, gMin)
          Swarm[i,0]= np.random.rand()*(cMax-cMin)+cMin  #  Penalty parameter (C)
          Swarm[i,1]= np.random.rand()*(gMax-gMin)+gMin #  RBF Kernel (Sigma)
          
          Swarm[i,0] = max(Swarm[i,0],cMin)
          Swarm[i,0] = min(Swarm[i,0],cMax)
          Swarm[i,1] = max(Swarm[i,1],gMin)
          Swarm[i,1] = min(Swarm[i,1],gMax)

          
        #   # #Update Position Bounds
        #   # R[p][0]= max(R[p][0],cMin)
        #   # R[p][0]= min(R[p][0],cMax)

        #   # R[p][1]= max(R[p][1], gMin)
        #   # R[p][1]= min(R[p][1], gMax)



        
        parameter_candidates=[{'C':list(Swarm[:,0]) , 'gamma': list(Swarm[:,1]), 'kernel': ['rbf']}]

        clf = RandomizedSearchCV(SVC(), parameter_candidates, n_iter=1,  n_jobs=-1, random_state=random_s)
        clf.fit(x_train, y_train)
        # clf.fit(Xtrain, ytrain) 

        svclassifier = SVC(kernel='rbf', gamma = clf.best_estimator_.gamma, C = clf.best_estimator_.C ,verbose=0, random_state=random_s)
  
        svclassifier.fit(x_train, y_train)
        ypred = svclassifier.predict(x_validation)
        ValidationError=sum(ypred==y_validation)*100/y_validation.shape[0]
        Fval[iter]= 100 - ValidationError

            
        if Fval[iter]<PreviouBest[iter,2]:
          PreviouBest[iter,0]=clf.best_estimator_.C # Update the position of the first dimension
          PreviouBest[iter,1]=clf.best_estimator_.gamma # Update the position of the second dimension
          PreviouBest[iter,2]=Fval[iter]          # Update best value

        #calculate global solu and position for current Run 
        if Fval[iter] < gBestValue:
          gBestValue = Fval[iter]
          gBestPos[0]   = clf.best_estimator_.C
          gBestPos[1]   = clf.best_estimator_.gamma



  
      print('fold_N = ', fold_N , ' Best value of C= ', gBestPos[0],'  Best value of Sigma=',gBestPos[1],  'fmin= ', gBestValue) 
      # ResultsValidation[r][fold_N]=gBestValue
      # fold_best_postion[fold_N]= gBestPos

      svclassifier = SVC(kernel='rbf', gamma = gBestPos[1], C = gBestPos[0] ,verbose=0, random_state=random_s)
      svclassifier.fit(x_train, y_train)
      ypred = svclassifier.predict(X_test)
    
      fpr, tpr, t = roc_curve(y_test, ypred)
      #print('fpr ',fpr) 
      #print('tpr',tpr)
      tprs.append(interp(mean_fpr, fpr, tpr))
      roc_auc = auc(fpr, tpr)
      aucs.append(roc_auc)
      #plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))
      i= i+1
      conf_matrix = confusion_matrix(y_test, ypred)
    
      TP.append(conf_matrix[1][1])
      TN.append(conf_matrix[0][0])
      FP.append(conf_matrix[0][1])
      FN.append(conf_matrix[1][0])
      conf_sensitivity = (conf_matrix[1][1] / float(conf_matrix[1][1] + conf_matrix[1][0]))
      TPR=conf_sensitivity
      Sensitifity[fold_N]=conf_sensitivity
      conf_specificity = (conf_matrix[0][0] / float(conf_matrix[0][0] + conf_matrix[0][1]))
      FPR= (1 - conf_specificity)
      Specificity[fold_N]=conf_specificity
      auC= (float(1+TPR - FPR)/2)
      AUC[fold_N]=auC
    
      #calculate Error ratio 
      TestingError=sum(ypred==y_test)*100/y_test.shape[0]
      #ResultsTestingAcc[r][fold_N]=100-TestingError;
      #SV[r]= svclassifier.n_support_
      #print('fold_N testing error ', 100-TestingError)
      fold_N = fold_N +1
  
    print (d, "AUC = %.2f SenStd = %.2f " % (AUC.mean(), AUC.std()))
    print (d, "Sen = %.2f SenStd = %.2f " % (Sensitifity.mean(), Sensitifity.std()))
    print (d, "Spe = %.2f SenStd = %.2f " % (Specificity.mean(), Specificity.std()))
    print('AUC' ,AUC)
    print('Sensitifity', Sensitifity)
    print('Specificity' ,Specificity)
    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    std_auc = np.std(aucs)
    plt.plot(mean_fpr, mean_tpr,
             label=d +' Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc),
             lw=2, alpha=.8)



    std_tpr = np.std(tprs, axis=0)
    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2)
print('******************************************')
end = time.time()
temp = end-start
print(temp)
hours = temp//3600
temp = temp - 3600*hours
minutes = temp//60
seconds = temp - 60*minutes
print('%d:%d:%d' %(hours,minutes,seconds))
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Optimize SVM (rbf)kernel C and gamma Using RandomizedSearchCV results')
plt.legend(loc="lower right")
plt.show()

"""# ElasticNet"""

resultsagg = pd.DataFrame()
data_set_name=['alon.tar.gz']
# data_set_name=['chin.tar.gz','alon.tar.gz','chowdary.tar.gz','golub.tar.gz',
#                'singh.tar.gz','west.tar.gz']
plt.figure(figsize=(8,8))
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',
         label='Luck', alpha=.8)    
import time
start = time.time()
for i in data_set_name:
  d= i.split('.')[0]
  print(d)
  file='/content/drive/My Drive/Colab Notebooks/data/'+i
  datasetname=d

  min_max_scaler = MinMaxScaler()
  with tarfile.open(file, "r:*") as tar:
    csv_path = tar.getnames()
    if(d == 'singh' or d == 'west'):
      labels = pd.read_csv(tar.extractfile(csv_path[1]), header=None)
      feature = pd.read_csv(tar.extractfile(csv_path[0]), header=None)

    else:
      labels = pd.read_csv(tar.extractfile(csv_path[0]), header=None)
      feature = pd.read_csv(tar.extractfile(csv_path[1]), header=None)
  headers = list(feature.columns)
  features = np.asarray(feature.values)
  features = min_max_scaler.fit_transform(feature)
  labels = np.transpose(np.asarray(labels.values.ravel() - 1, dtype=int))
  print(features.shape, labels.shape)

  counter = Counter(labels)
  print(counter)
  
  Max_iterations=50  # Maximum Number of Iterations
  correction_factor = 2.0 # Correction factor
  inertia = 1.0 # Ineritia Coeffecient
  swarm_size = 20 # Number of particles
  LB=1e-4*np.ones((1,2))
  UB=1e-3*np.ones((1,2))

  Xrange=UB[0,0]-LB[0,0]
  Yrange=UB[0,1]-LB[0,1]
  Dim=UB.shape[1]

  fold_N=0  # Number of runs
  
  #evaluated matrix
  max_fold=10
  Accuracy=np.zeros((max_fold))
  Precision=np.zeros((max_fold))
  Recall=np.zeros((max_fold))
  ResultsTestingAcc = np.zeros((max_fold))
  TP=[]
  FP=[]
  TN=[]
  FN=[]

  Specificity = []
  Sensitifity = []
  AUC = []

  tprs = []
  aucs = []
  mean_fpr = np.linspace(0, 1, 100)
  BestAlpha=np.zeros((max_fold))

  NoRun=3
  
  for r in range(0,NoRun):
    fold_N=0
    i=0
    lines=["-",":","--","-."]
    cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=random_s)

    for train_index, test_index in cv.split(features,labels):
      Swarm=np.zeros((swarm_size,2))
      PreviouBest=float('inf')* np.ones((Max_iterations,3))
      # Initial best value so far
      # GlobalBestSolution=float('inf')
      # BestSolns=float('inf')* np.ones((swarm_size,2))
      gBestPos   = [0] * 1
      gBestValue = float("inf") 
      #store Global Best Position and Solution For Current Run 
    
	    
      # Initial velocity
      # Velocity=np.zeros((swarm_size,2))
      Fval=np.zeros((Max_iterations))
      Xtrain, Xtest =  features[train_index], features[test_index]
      ytrain, ytest = labels[train_index], labels[test_index]

      

      # for iter in range(Max_iterations):
        # Calculating fitness value for all particles    
        #print('Iteration Number ', iter )
       
      for i in range(swarm_size):
                             
          #R = initPosition (R, Np, Nd, cMin, cMax, gMax, gMin)
          Swarm[i,0]= np.random.random()*Xrange+LB[0,0]  #  Penalty parameter (C)
          Swarm[i,1]= np.random.random()*Yrange+LB[0,1] #  RBF Kernel (Sigma)
          
          Swarm[i,0] = max(Swarm[i,0],LB[0,0])
          Swarm[i,0] = min(Swarm[i,0],UB[0,0])
          Swarm[i,1] = max(Swarm[i,1],LB[0,1])
          Swarm[i,1] = min(Swarm[i,1],UB[0,1])
      
        #clf = GridSearchCV(estimator=svm.SVC(),cv=1, param_grid=parameter_candidates, n_jobs=-1)
      clf = ElasticNetCV(alphas=list(Swarm[:,0]), tol=0.0001, cv=None, 
                         copy_X=True, verbose=0, n_jobs=-1, positive=False, random_state=random_s, selection='cyclic')
      # clf = RandomizedSearchCV(ElasticNet(), parameter_grid, n_iter=1, scoring='r2', n_jobs=-1, random_state=random_s)
      #clf.fit(x_train, y_train)
      clf.fit(Xtrain, ytrain) 
      #claculate Fitness 
      #print("iteration ", iter , 'Param', clf.best_estimator_.alpha)
      elasticNet=ElasticNet(alpha=clf.alpha_,random_state=random_s)
      elasticNet.fit(Xtrain,ytrain)
      data_el= elasticNet.coef_
      gains = np.asarray(data_el)
      indexes = np.where(gains != 0)[0]
      #print(indexes.shape)
      if(indexes.shape[0]== 0):
        continue

      xtrain = Xtrain[:, indexes]
      xtest = Xtest[:, indexes]
      #xtrain, xvalidation, ytrain, yvalidation = train_test_split(xtrain,y_train,test_size = 0.11)



      svclassifier = SVC(kernel='rbf',verbose=0, random_state=random_s)  
      svclassifier.fit(xtrain, ytrain)
      ypred = svclassifier.predict(xtest)
            
      ValidationError=sum(ypred==ytest)*100/xtest.shape[0]
      


      print('fold_N = ', fold_N , ' Best value of Alpha = ', clf.alpha_,   'with fitness value = ', ValidationError)
      # BestAlpha[fold_N]= gBestPos
      #ResultsValidation[r][fold_N]=gBestValue
      #fold_best_postion[fold_N]= gBestPos

      F_train, F_test =  features[train_index], features[test_index]
      z_train, z_test = labels[train_index], labels[test_index]

      #claculate Fitness 
      elasticNet=ElasticNet(alpha=clf.alpha_,random_state=random_s)
      elasticNet.fit(F_train,z_train)
      data_el= elasticNet.coef_
      gains = np.asarray(data_el)
      indexes = np.where(gains != 0)[0]
      #print(indexes.shape)
      if(indexes.shape[0]== 0):
        continue

      f_train = F_train[:, indexes]
      f_test = F_test[:, indexes]
    
      svclassifier = SVC(kernel='rbf', verbose=0, random_state=random_s)
      svclassifier.fit(f_train, z_train)
      ypred = svclassifier.predict(f_test)
      #y_true = y_test
      TestingError=sum(ypred==z_test)*100/z_test.shape[0]
      print('TestingError = ', TestingError)
      ResultsTestingAcc[fold_N]=100-TestingError;
      #SV[r]= svclassifier.n_support_
      #print('fold_N testing error ', 100-TestingError)
      fpr, tpr, t = roc_curve(z_test, ypred)
      #print('fpr ',fpr) 
      #print('tpr',tpr)
      tprs.append(interp(mean_fpr, fpr, tpr))
      roc_auc = auc(fpr, tpr)
      aucs.append(roc_auc)
      # i= i+1
      conf_matrix = confusion_matrix(z_test, ypred)
      # TP.append(conf_matrix[1][1])
      # TN.append(conf_matrix[0][0])
      # FP.append(conf_matrix[0][1])
      # FN.append(conf_matrix[1][0])
      # conf_sensitivity = (conf_matrix[1][1] / float(conf_matrix[1][1] + conf_matrix[1][0]))
      # TPR=conf_sensitivity
      sensitivity1 = conf_matrix[0,0]/(conf_matrix[0,0]+conf_matrix[0,1])

      Sensitifity[fold_N]=sensitivity1
      # conf_specificity = (conf_matrix[0][0] / float(conf_matrix[0][0] + conf_matrix[0][1]))
      # FPR= (1 - conf_specificity)
      specificity1 = conf_matrix[1,1]/(conf_matrix[1,0]+conf_matrix[1,1])

      Specificity[fold_N]=specificity1
      # auC= (float(1+TPR - FPR)/2)
      AUC[fold_N]=roc_auc
      fold_N = fold_N +1
      

  
      
  
    print (d, "AUC = %.2f SenStd = %.2f " % (AUC.mean(), AUC.std()))
    print (d, "Sen = %.2f SenStd = %.2f " % (Sensitifity.mean(), Sensitifity.std()))
    print (d, "Spe = %.2f SenStd = %.2f " % (Specificity.mean(), Specificity.std()))
    print('AUC' ,AUC)
    print('Sensitifity', Sensitifity)
    print('Specificity' ,Specificity)
    
    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    std_auc = np.std(aucs)
    if(i% 2 == 0):
      plt.plot(mean_fpr, mean_tpr,
               label=d +' Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc),
               lw=2, alpha=.8, linestyle=lines[i])
    else:
      plt.plot(mean_fpr, mean_tpr,
               label=d +' Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc),
               lw=2, alpha=.8)

    i = i +1

    std_tpr = np.std(tprs, axis=0)
    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2)
print('******************************************')
end = time.time()
temp = end-start
print(temp)
hours = temp//3600
temp = temp - 3600*hours
minutes = temp//60
seconds = temp - 60*minutes
print('%d:%d:%d' %(hours,minutes,seconds))
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("RandomizedSearchCV RS-ENSVM results  ")
plt.legend(loc="lower right")
plt.show()

df =pd.ExcelFile("/content/drive/My Drive/Colab Notebooks/data/iris.xlsx")
df = pd.read_excel(df, 'iris',header=None)
df = df.iloc[:,-1]
df

"""# ELENSVM"""

# 'golub.tar.gz','gordon.tar.gz','singh.tar.gz','west.tar.gz'
# 'chin.tar.gz','alon.tar.gz','chowdary.tar.gz'
# ,'singh.tar.gz','west.tar.gz'
# 'gordon.tar.gz','golub.tar.gz'
data_set_name=['singh.tar.gz']
# plt.figure(figsize=(8,8))
# plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',
#          label='Luck', alpha=.8)    
import time
start = time.time()
for i in data_set_name:
  df =pd.ExcelFile("/content/drive/My Drive/Colab Notebooks/data/glass.xlsx")
  df = pd.read_excel(df, 'glass',header=None)
  feature = df.iloc[:,:-1]
  labels = df.iloc[:,-1]
  min_max_scaler = MinMaxScaler()
  
  headers = ['sepal_length','sepal_width','petal_length','sepal_width','class']
  features = np.asarray(feature.values)
  features = min_max_scaler.fit_transform(feature)
  label_encoder = LabelEncoder()
  labels= label_encoder.fit_transform(labels) 
  labels= labels.astype(dtype=float)
  # labels = np.transpose(np.asarray(labels.values.ravel() - 1, dtype=int))
  print(features.shape, labels.shape)

  counter = Counter(labels)
  print(counter)
  
  

  Max_iterations=50  # Maximum Number of Iterations
  correction_factor = 2.0 # Correction factor
  inertia = 1.0 # Ineritia Coeffecient
  swarm_size = 20 # Number of particles
  LB=1e-4*np.ones((1,2))
  UB=1e-3*np.ones((1,2))

  Xrange=UB[0,0]-LB[0,0]
  Yrange=UB[0,1]-LB[0,1]
  Dim=UB.shape[1]

  fold_N=0  # Number of runs
  cv = StratifiedKFold(n_splits=10,shuffle=False)

  #evaluated matrix
  max_fold=10
  Accuracy=np.zeros((max_fold))
  Precision=np.zeros((max_fold))
  Recall=np.zeros((max_fold))
  ResultsTestingAcc = np.zeros((max_fold))
  TP=[]
  FP=[]
  TN=[]
  FN=[]

  Specificity = np.zeros((max_fold))
  Sensitifity = np.zeros((max_fold))
  AUC = np.zeros((max_fold))

  tprs = []
  aucs = []
  mean_fpr = np.linspace(0, 1, 100)
  BestAlpha=np.zeros((max_fold))

  NoRun=1
  
  for r in range(0,NoRun):
    fold_N=0
    i=0
    lines=["-",":","--","-."]
    for train_index, test_index in cv.split(features,labels):
      Swarm=np.zeros((swarm_size,2))
      PreviouBest=float('inf')* np.ones((Max_iterations,3))
      # Initial best value so far
      # GlobalBestSolution=float('inf')
      # BestSolns=float('inf')* np.ones((swarm_size,2))
      gBestPos   = [0] * 1
      gBestValue = float("inf") 
      #store Global Best Position and Solution For Current Run 
    
	    
      # Initial velocity
      # Velocity=np.zeros((swarm_size,2))
      Fval=np.zeros((Max_iterations))
      Xtrain, Xtest =  features[train_index], features[test_index]
      ytrain, ytest = labels[train_index], labels[test_index]

      

      for iter in range(Max_iterations):
        # Calculating fitness value for all particles    
        #print('Iteration Number ', iter )
       
        for i in range(swarm_size):
                      
          #R = initPosition (R, Np, Nd, cMin, cMax, gMax, gMin)
          Swarm[i,0]= np.random.random()*Xrange+LB[0,0]  #  Penalty parameter (C)
          Swarm[i,1]= np.random.random()*Yrange+LB[0,1] #  RBF Kernel (Sigma)
          
          Swarm[i,0] = max(Swarm[i,0],LB[0,0])
          Swarm[i,0] = min(Swarm[i,0],UB[0,0])
          Swarm[i,1] = max(Swarm[i,1],LB[0,1])
          Swarm[i,1] = min(Swarm[i,1],UB[0,1])
      
        #clf = GridSearchCV(estimator=svm.SVC(),cv=1, param_grid=parameter_candidates, n_jobs=-1)
        clf = ElasticNetCV(alphas=list(Swarm[:,0]), tol=0.0001, cv=None, max_iter=1,
                           copy_X=True, verbose=0, n_jobs=-1, positive=False, random_state=random_s, selection='cyclic')
        # clf = RandomizedSearchCV(ElasticNet(), parameter_grid, n_iter=1, scoring='r2', n_jobs=-1, random_state=random_s)
        #clf.fit(x_train, y_train)
        clf.fit(Xtrain, ytrain) 
        #claculate Fitness 
        #print("iteration ", iter , 'Param', clf.best_estimator_.alpha)
        elasticNet=ElasticNet(alpha=clf.alpha_,random_state=random_s)
        elasticNet.fit(Xtrain,ytrain)
        data_el= elasticNet.coef_
        gains = np.asarray(data_el)
        indexes = np.where(gains != 0)[0]
        #print(indexes.shape)
        if(indexes.shape[0]== 0):
          continue

        xtrain = Xtrain[:, indexes]
        xtest = Xtest[:, indexes]
        #xtrain, xvalidation, ytrain, yvalidation = train_test_split(xtrain,y_train,test_size = 0.11)



        svclassifier = SVC(kernel='rbf',verbose=0, random_state=random_s)  
        svclassifier.fit(xtrain, ytrain)
        ypred = svclassifier.predict(xtest)
            
        ValidationError=sum(ypred==ytest)*100/xtest.shape[0]
        Fval[iter]= 100 - ValidationError

            
        if Fval[iter]<PreviouBest[iter,2]:
          PreviouBest[iter,0]=clf.alpha_# Update the position of the first dimension
          PreviouBest[iter,1]=0 # Update the position of the second dimension
          PreviouBest[iter,2]=Fval[iter]          # Update best value

        #calculate global solu and position for current Run 
        if Fval[iter] < gBestValue:
          gBestValue = Fval[iter]
          gBestPos   = clf.alpha_  




      print('fold_N = ', fold_N , ' Best value of Alpha = ', gBestPos,   'with fitness value = ', gBestValue)
      BestAlpha[fold_N]= gBestPos
      #ResultsValidation[r][fold_N]=gBestValue
      #fold_best_postion[fold_N]= gBestPos

      F_train, F_test =  features[train_index], features[test_index]
      z_train, z_test = labels[train_index], labels[test_index]

      #claculate Fitness 
      elasticNet=ElasticNet(alpha=gBestPos,random_state=random_s)
      elasticNet.fit(F_train,z_train)
      data_el= elasticNet.coef_
      gains = np.asarray(data_el)
      indexes = np.where(gains != 0)[0]
      print('features importance : ',indexes.shape)
      if(indexes.shape[0]== 0):
        continue

      f_train = F_train[:, indexes]
      f_test = F_test[:, indexes]
    
      svclassifier = SVC(kernel='rbf', verbose=0, random_state=random_s)
      svclassifier.fit(f_train, z_train)
      ypred = svclassifier.predict(f_test)
      #y_true = y_test
      TestingError=sum(ypred==z_test)*100/z_test.shape[0]
      print('TestingError = ', TestingError)
      ResultsTestingAcc[fold_N]=TestingError;
      
      fold_N = fold_N +1
      

print('******************************************')
end = time.time()
temp = end-start
print(temp)
hours = temp//3600
temp = temp - 3600*hours
minutes = temp//60
seconds = temp - 60*minutes
print('%d:%d:%d' %(hours,minutes,seconds)) 
print('testing acc : ')
print(ResultsTestingAcc.mean(), ResultsTestingAcc.std())

"""# SSD"""

#Social Ski-Driver (SSD) optimization algorithm
# More details about the algorithm are in [please cite the original paper (below)]
#Alaa Tharwat, Thomas Gabel, "Parameters optimization of support vector machines for imbalanced data using social ski driver algorithm"
#Neural Computing and Applications, pp. 1-14, 2019
#Tharwat, A. & Gabel, T. Neural Comput & Applic (2019). https://doi.org/10.1007/s00521-019-04159-z
#file='/content/drive/My Drive/Colab Notebooks/data/golub.tar.gz'

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
from sklearn.metrics import roc_curve, auc
from numpy import interp

from  warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)
import time

df =pd.ExcelFile("/content/drive/My Drive/Colab Notebooks/data/glass.xlsx")
df = pd.read_excel(df, 'glass',header=None)
feature = df.iloc[:,:-1]
labels = df.iloc[:,-1]

min_max_scaler = MinMaxScaler()

headers = ['sepal_length','sepal_width','petal_length','sepal_width','class']
features = np.asarray(feature.values)
features = min_max_scaler.fit_transform(feature)
label_encoder = LabelEncoder()
labels= label_encoder.fit_transform(labels) 
labels= labels.astype(dtype=float)
print(features.shape, labels.shape)

counter = Counter(labels)
print(counter)





Max_iterations=50  # Maximum Number of Iterations
correction_factor = 2.0 # Correction factor
inertia = 1.0 # Ineritia Coeffecient
swarm_size = 20 # Number of particles
LB=1e-4*np.ones((1,2))
UB=1e-3*np.ones((1,2))

Xrange=UB[0,0]-LB[0,0]
Yrange=UB[0,1]-LB[0,1]
Dim=UB.shape[1]

fold_N=0  # Number of runs
random_s = np.random.RandomState(10)
cv = StratifiedKFold(n_splits=10,shuffle=False)

#evaluated matrix
max_fold=10
Accuracy=np.zeros((max_fold))
Precision=np.zeros((max_fold))
Recall=np.zeros((max_fold))
ResultsTestingAcc = np.zeros((max_fold))
TP=[]
FP=[]
TN=[]
FN=[]

Specificity = np.zeros((max_fold))
Sensitifity = np.zeros((max_fold))
AUC = np.zeros((max_fold))

tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)
BestAlpha=np.zeros((max_fold))

# Initial Positions
ConvergenceCurve=np.zeros((Max_iterations,max_fold))
start = time.time()

for train_index, test_index in cv.split(features,labels):
    Swarm=np.zeros((swarm_size,2))
    PreviouBest=float('inf')* np.ones((swarm_size,3))
    # Initial best value so far
    GlobalBestSolution=float('inf')
    BestSolns=float('inf')* np.ones((swarm_size,2))
    gBestPos   = [0] * Dim
    gBestValue = float("inf") 
    #store Global Best Position and Solution For Current Run 
    
	    
    # Initial velocity
    Velocity=np.zeros((swarm_size,2))
    Fval=np.zeros((swarm_size))
    for i in range(swarm_size):
        Swarm[i,0]= np.random.random()*Xrange+LB[0,0]
        Swarm[i,1]= np.random.random()*Yrange+LB[0,1]
        #print(Swarm)
        

        X_train, X_test =  features[train_index], features[test_index]
        y_train, y_test = labels[train_index], labels[test_index]
        
        #claculate Fitness 
        elasticNet=ElasticNet(alpha=Swarm[i][0], random_state=random_s)
        elasticNet.fit(X_train,y_train)
        data_el= elasticNet.coef_
        gains = np.asarray(data_el)
        indexes = np.where(gains != 0)[0]
        #print(indexes.shape)
        if(indexes.shape[0]== 0):
          continue

        x_trainx = X_train[:, indexes]
        x_testx = X_test[:, indexes]
        #x_train, x_validation, y_train, y_validation = train_test_split(x_train,y_train,test_size = 0.11)

        svclassifier = SVC(kernel='rbf',verbose=0, random_state=random_s)  
        svclassifier.fit(x_trainx, y_train)
        ypred = svclassifier.predict(x_testx)
        # elasticNet.fit(x_train, y_train)
        # ypred = elasticNet.predict(x_test)
        # ypred = np.round(ypred,decimals=0)
        # ypred= ypred.astype(int)
        ValidationError=sum(ypred==y_test)*100/y_test.shape[0]
        Fval[i]= 100 - ValidationError
        
        if Fval[i]<PreviouBest[i,2]:
          PreviouBest[i,0]=Swarm[i,0] # Update the position of the first dimension
          PreviouBest[i,1]=Swarm[i,1] # Update the position of the second dimension
          PreviouBest[i,2]=Fval[i]          # Update best value
        #calculate global solu and position for current Run 
        if Fval[i] < gBestValue:
          gBestValue = Fval[i]
          gBestPos   = Swarm[i]  



    for iter in range(Max_iterations):
        # Calculating fitness value for all particles    
        for i in range(swarm_size):
            Swarm[i,0]=Swarm[i,0]+Velocity[i,0]
            Swarm[i,1]=Swarm[i,1]+Velocity[i,1]
            
            # Update Position Bounds
            Swarm[i,0] = max(Swarm[i,0],LB[0,0])
            Swarm[i,0] = min(Swarm[i,0],UB[0,0])
            Swarm[i,1] = max(Swarm[i,1],LB[0,1])
            Swarm[i,1] = min(Swarm[i,1],UB[0,1])
            

            

            Xtrain, Xtest =  features[train_index], features[test_index]
            ytrain, ytest = labels[train_index], labels[test_index]

            #claculate Fitness 
            elasticNet=ElasticNet(alpha=Swarm[i][0],random_state=random_s)
            elasticNet.fit(X_train,y_train)
            data_el= elasticNet.coef_
            gains = np.asarray(data_el)
            indexes = np.where(gains != 0)[0]
            #print(indexes.shape)
            if(indexes.shape[0]== 0): 
              continue

            xtrain = X_train[:, indexes]
            xtest = X_test[:, indexes]
            #xtrain, xvalidation, ytrain, yvalidation = train_test_split(xtrain,y_train,test_size = 0.11)



            svclassifier = SVC(kernel='rbf',verbose=0, random_state=random_s)  
            svclassifier.fit(xtrain, ytrain)
            ypred = svclassifier.predict(xtest)
            # elasticNet.fit(x_train, y_train)
            # ypred = elasticNet.predict(x_test)
            # ypred = np.round(ypred,decimals=0)
            # ypred= ypred.astype(int)
            ValidationError=sum(ypred==ytest)*100/xtest.shape[0]
            Fval[i]= 100 - ValidationError

            
            if Fval[i]<PreviouBest[i,2]:
                PreviouBest[i,0]=Swarm[i,0] # Update the position of the first dimension
                PreviouBest[i,1]=Swarm[i,1] # Update the position of the second dimension
                PreviouBest[i,2]=Fval[i]          # Update best value

            #calculate global solu and position for current Run 
            if Fval[i] < gBestValue:
              gBestValue = Fval[i]
              gBestPos   = Swarm[i]  
                
        a=2.0-iter*(float((2)/float(Max_iterations))) # a decreases linearly fron 2 to 0
        
        # Search for the global best solution
        (Gbest,idxGbest) = min((v,i) for i,v in enumerate(PreviouBest[:,2]))
        #    or
        #    Gbest= PreviouBest[:,2].min()
        #    idxGbest=np.argmin(PreviouBest[:,2])
        
        SortedElements=np.sort(PreviouBest[:,2])
        idxSortedElements=np.argsort(PreviouBest[:,2])
        # Find the mean of the best three solutions
        M=np.zeros((Dim))
        for i in range(Dim):
            M[i]=np.mean(Swarm[idxSortedElements[0:3],i])
    
        r1=np.random.random()  #r1 is a random number in [0,1]
        r2=np.random.random()  # r2 is a random number in [0,1]
        
        A1=2*a*r1-a     
        C1=2*r2         
        
        # Updating velocity vectors        
        for i in range(swarm_size):

            if np.random.random()<0.5:
                for j in range(Dim):
                    Velocity[i,j]=(a)*np.sin(np.random.random())*(PreviouBest[i,j] - Swarm[i,j])+(2.0-a)*np.sin(np.random.random())*(M[j]- Swarm[i,j])   # velocity component
            else:
                for j in range(Dim):
                    Velocity[i,j]=(a)*np.cos(np.random.random())*(PreviouBest[i,j] - Swarm[i,j])+(2.0-a)*np.cos(np.random.random())*(M[j]- Swarm[i,j])   # velocity component
        
        ConvergenceCurve[iter,fold_N]=SortedElements[0]

    print('fold_N = ', fold_N , ' Best value of Alpha = ', gBestPos[0],   'with fitness value = ', gBestValue)
    BestAlpha[fold_N]= gBestPos[0]
    #ResultsValidation[r][fold_N]=gBestValue
    #fold_best_postion[fold_N]= gBestPos

    F_train, F_test =  features[train_index], features[test_index]
    z_train, z_test = labels[train_index], labels[test_index]

    #claculate Fitness 
    elasticNet=ElasticNet(alpha=gBestPos[0],random_state=random_s)
    elasticNet.fit(F_train,z_train)
    data_el= elasticNet.coef_
    gains = np.asarray(data_el)
    indexes = np.where(gains != 0)[0]
    print(indexes.shape)
    if(indexes.shape[0]== 0):
      continue

    f_train = F_train[:, indexes]
    f_test = F_test[:, indexes]
    # x_train, x_validation, y_train, y_validation = train_test_split(x_train,y_train,test_size = 0.11)
    # elasticNet=ElasticNet(alpha=gBestPos[0])
    # gains = np.asarray(data_el)
    # indexes = np.where(gains != 0)[0]
    # #print(indexes.shape)
    # if(indexes.shape[0]== 0):
    #   continue

    # X_train, X_test =  features[train_index], features[test_index]
    # y_train, y_test = labels[train_index], labels[test_index]
    # x_train = X_train[:, indexes]
    # x_test = X_test[:, indexes]
    # x_train, x_validation, y_train, y_validation = train_test_split(x_train,y_train,test_size = 0.11)
    # elasticNet.fit(x_train,y_train)

    # ypred = elasticNet.predict(x_test)
    svclassifier = SVC(kernel='rbf', verbose=0, random_state=random_s)
    svclassifier.fit(f_train, z_train)
    ypred = svclassifier.predict(f_test)
    #y_true = y_test
    TestingError=sum(ypred==z_test)*100/z_test.shape[0]
    print('TestingError = ', TestingError)
    ResultsTestingAcc[fold_N]=TestingError;

    fold_N = fold_N +1
  
print('******************************************')
end = time.time()
temp = end-start
print(temp)
hours = temp//3600
temp = temp - 3600*hours
minutes = temp//60
seconds = temp - 60*minutes
print('%d:%d:%d' %(hours,minutes,seconds))
print('testing acc : ')
print(ResultsTestingAcc.mean(), ResultsTestingAcc.std())

"""# RSENSVM"""

# 'golub.tar.gz','gordon.tar.gz','singh.tar.gz','west.tar.gz'
# 'chin.tar.gz','alon.tar.gz','chowdary.tar.gz'
# ,'singh.tar.gz','west.tar.gz'
# 'gordon.tar.gz','golub.tar.gz'
data_set_name=['gordon.tar.gz']
# data_set_name=['west.tar.gz']
# plt.figure(figsize=(8,8))
# plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',
#          label='Luck', alpha=.8)    
# AUCDF = pd.DataFrame()
# SENDF = pd.DataFrame()
# SPECDF = pd.DataFrame()
import time
start = time.time()

for i in data_set_name:
  df =pd.ExcelFile("/content/drive/My Drive/Colab Notebooks/data/glass.xlsx")
  df = pd.read_excel(df, 'glass',header=None)
  feature = df.iloc[:,:-1]
  labels = df.iloc[:,-1]
  min_max_scaler = MinMaxScaler()
  
  headers = ['sepal_length','sepal_width','petal_length','sepal_width','class']
  features = np.asarray(feature.values)
  features = min_max_scaler.fit_transform(feature)
  label_encoder = LabelEncoder()
  labels= label_encoder.fit_transform(labels) 
  labels= labels.astype(dtype=float)
  # labels = np.transpose(np.asarray(labels.values.ravel() - 1, dtype=int))
  print(features.shape, labels.shape)

  
  counter = Counter(labels)
  print(counter)
  
  Max_iterations=50  # Maximum Number of Iterations
  correction_factor = 2.0 # Correction factor
  inertia = 1.0 # Ineritia Coeffecient
  swarm_size = 20 # Number of particles
  LB=1e-4*np.ones((1,2))
  UB=1e-3*np.ones((1,2))

  Xrange=UB[0,0]-LB[0,0]
  Yrange=UB[0,1]-LB[0,1]
  Dim=UB.shape[1]

  fold_N=0  # Number of runs
  cv = StratifiedKFold(n_splits=10,shuffle=False)

  #evaluated matrix
  max_fold=10
  Accuracy=np.zeros((max_fold))
  Precision=np.zeros((max_fold))
  Recall=np.zeros((max_fold))
  ResultsTestingAcc = np.zeros((max_fold))
  TP=[]
  FP=[]
  TN=[]
  FN=[]

  Specificity = np.zeros((max_fold))
  Sensitifity = np.zeros((max_fold))
  AUC = np.zeros((max_fold))

  tprs = []
  aucs = []
  mean_fpr = np.linspace(0, 1, 100)
  BestAlpha=np.zeros((max_fold))

  NoRun=1
  # AUCmatrix=np.zeros((Max_iterations,max_fold))
  # SENmatrix=np.zeros((Max_iterations,max_fold))
  # SPECmatrix=np.zeros((Max_iterations,max_fold))


  for r in range(0,NoRun):
    fold_N=0
    i=0
    lines=["-",":","--","-."]
    for train_index, test_index in cv.split(features,labels):
      Swarm=np.zeros((swarm_size,2))
      PreviouBest=float('inf')* np.ones((Max_iterations,3))
      # Initial best value so far
      # GlobalBestSolution=float('inf')
      # BestSolns=float('inf')* np.ones((swarm_size,2))
      gBestPos   = [0] * 1
      gBestValue = float("inf") 
      #store Global Best Position and Solution For Current Run 
    
	    
      # Initial velocity
      # Velocity=np.zeros((swarm_size,2))
      Fval=np.zeros((Max_iterations))

      

      for iter in range(Max_iterations):
        # Calculating fitness value for all particles    
        #print('Iteration Number ', iter )
        Xtrain, Xtest =  features[train_index], features[test_index]
        ytrain, ytest = labels[train_index], labels[test_index]

        parameter_grid=[{'alpha': loguniform(1e-4, 1e-3)}]
      
        #clf = GridSearchCV(estimator=svm.SVC(),cv=1, param_grid=parameter_candidates, n_jobs=-1)
        clf = RandomizedSearchCV(ElasticNet(), parameter_grid, n_iter=1, scoring='r2', n_jobs=-1, random_state=random_s)
        #clf.fit(x_train, y_train)
        clf.fit(Xtrain, ytrain) 
        #claculate Fitness 
        #print("iteration ", iter , 'Param', clf.best_estimator_.alpha)
        elasticNet=ElasticNet(alpha=clf.best_estimator_.alpha,random_state=random_s)
        elasticNet.fit(Xtrain,ytrain)
        data_el= elasticNet.coef_
        gains = np.asarray(data_el)
        indexes = np.where(gains != 0)[0]
        # print('indexes type', type(indexes) , indexes.shape)
        if(indexes.shape[0]== 0):
          continue

        xtrain = Xtrain[:, indexes]
        xtest = Xtest[:, indexes]
        #xtrain, xvalidation, ytrain, yvalidation = train_test_split(xtrain,y_train,test_size = 0.11)



        svclassifier = SVC(kernel='rbf',verbose=0, random_state=random_s)  
        svclassifier.fit(xtrain, ytrain)
        ypred = svclassifier.predict(xtest)
        # #calculate AUC SEN SPEC 
        # conf_matrix = confusion_matrix(ytest, ypred)
        # TP.append(conf_matrix[1][1])
        # TN.append(conf_matrix[0][0])
        # FP.append(conf_matrix[0][1])
        # FN.append(conf_matrix[1][0])
        # conf_sensitivity = (conf_matrix[1][1] / float(conf_matrix[1][1] + conf_matrix[1][0]))
        # TPR=conf_sensitivity
        # SENmatrix[iter][fold_N]=conf_sensitivity
        # conf_specificity = (conf_matrix[0][0] / float(conf_matrix[0][0] + conf_matrix[0][1]))
        # FPR= (1 - conf_specificity)
        # SPECmatrix[iter][fold_N]=conf_specificity
        # auC= (float(1+TPR - FPR)/2)
        # AUCmatrix[iter][fold_N]=auC






            
        ValidationError=sum(ypred==ytest)*100/xtest.shape[0]
        Fval[iter]= 100 - ValidationError

            
        if Fval[iter]<PreviouBest[iter,2]:
          PreviouBest[iter,0]=clf.best_estimator_.alpha # Update the position of the first dimension
          PreviouBest[iter,1]=0 # Update the position of the second dimension
          PreviouBest[iter,2]=Fval[iter]          # Update best value

        #calculate global solu and position for current Run 
        if Fval[iter] < gBestValue:
          gBestValue = Fval[iter]
          gBestPos   = clf.best_estimator_.alpha  




      print('fold_N = ', fold_N , ' Best value of Alpha = ', gBestPos,   'with fitness value = ', gBestValue)
      BestAlpha[fold_N]= gBestPos
      #ResultsValidation[r][fold_N]=gBestValue
      #fold_best_postion[fold_N]= gBestPos

      F_train, F_test =  features[train_index], features[test_index]
      z_train, z_test = labels[train_index], labels[test_index]

      #claculate Fitness 
      elasticNet=ElasticNet(alpha=gBestPos,random_state=random_s)
      elasticNet.fit(F_train,z_train)
      data_el= elasticNet.coef_
      gains = np.asarray(data_el)
      indexes = np.where(gains != 0)[0]
      print(indexes.shape)
      if(indexes.shape[0]== 0):
        continue

      f_train = F_train[:, indexes]
      f_test = F_test[:, indexes]
    
      svclassifier = SVC(kernel='rbf', verbose=0, random_state=random_s)
      svclassifier.fit(f_train, z_train)
      ypred = svclassifier.predict(f_test)
      #y_true = y_test
      TestingError=sum(ypred==z_test)*100/z_test.shape[0]
      print('TestingError = ', TestingError)
      ResultsTestingAcc[fold_N]=TestingError;
 
      fold_N = fold_N +1
      
print('******************************************')
end = time.time()
temp = end-start
print(temp)
hours = temp//3600
temp = temp - 3600*hours
minutes = temp//60
seconds = temp - 60*minutes
print('%d:%d:%d' %(hours,minutes,seconds))
print('testing acc : ')
print(ResultsTestingAcc.mean(), ResultsTestingAcc.std())

print('glass6')
print ("AUC = %.2f SenStd = %.2f " % (AUC.mean(), AUC.std()))
print ("Sen = %.2f SenStd = %.2f " % (Sensitifity.mean(), Sensitifity.std()))
print ("Spe = %.2f SenStd = %.2f " % (Specificity.mean(), Specificity.std()))
print('AUC' ,AUC)
print('Sensitifity', Sensitifity)
print('Specificity' ,Specificity)